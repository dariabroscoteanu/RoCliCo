{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMfT9qZwIHzP2UNUCI1r73l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Please fill the following paths."],"metadata":{"id":"Ah_JSYFfNGrn"}},{"cell_type":"code","source":["# Path to the test dataset, containing \"digi.json\" and \"libertatea.json\"\n","TEST_PATH = \"\"\n","\n","# Path to the saved Random Forest model\n","RFC_PATH = \"\"\n","\n","# Path to the saved lstm model\n","LSTM_PATH = \"\"\n","\n","# Path to the saved BERT model\n","BERT_PATH =  \"\"\n","\n","# Path to the saved Contrastive Learning model\n","CONTRASTIVE_PATH = \"\""],"metadata":{"id":"9lwbrL87NGWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Random Forest"],"metadata":{"id":"N4wXAININ8Eg"}},{"cell_type":"code","source":["! pip install stanza"],"metadata":{"id":"IQR-th-OVuPt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from string import punctuation\n","import re\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","from scipy import sparse\n","import pickle\n","import sklearn\n","\n","import stanza\n","stanza.download('ro')\n","\n","import nltk\n","from nltk.tokenize import sent_tokenize\n","\n","nlp = stanza.Pipeline('ro', processors='tokenize,pos', tokenize_no_ssplit=True)\n","nltk.download('punkt')\n","\n","\n","def preprocess(text):\n","    result = text.replace('/', \"\").replace('\\n', '')\n","    result = re.sub(r'[0-9]+', 'număr', result)\n","    result = re.sub(r'(\\w)(\\1{2,})', r'\\1', result)\n","    result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n","    result = result.lower()\n","    punctuations = punctuation + \"„”\"\n","    result = \"\".join(word for word in result if word not in punctuations)\n","    re.sub(r' +', ' ', result).lower().strip()\n","    return result\n","\n","\n","question_words = [\"ce\", \"cine\", \"cui\", \"care\", \"căruia\", \"căreia\", \"cărora\", \"căruia\", \"cât\", \"cît\", \"câți\", \"câtă\",\n","                  \"câte\", \"câtor\", \"cum\", \"oare\"]\n","\n","\n","def isquestion(text):\n","    result = text.lower().split()\n","    if result[0] in question_words:\n","        return 1\n","    else:\n","        return 0\n","\n","\n","# the stopwords list is taken from this url: https://countwordsfree.com/stopwords/romanian\n","# the words used in questions are removed from this list\n","romanian_stopwords = ['acea', 'aceasta', 'această', 'aceea', 'acei', 'aceia', 'acel', 'acela', 'acele', 'acelea', 'acest', 'acesta', 'aceste', 'acestea', 'aceşti', 'aceştia', 'acolo', 'acord', 'acum', 'ai', 'aia', 'aibă', 'aici', 'al', 'ăla', 'ale', 'alea', 'ălea', 'altceva', 'altcineva', 'am', 'ar', 'are', 'aş', 'aşadar', 'asemenea', 'asta', 'ăsta', 'astăzi', 'astea', 'ăstea', 'ăştia', 'asupra', 'aţi', 'au', 'avea', 'avem', 'aveţi', 'azi', 'bine', 'bucur', 'bună', 'ca', 'că', 'căci', 'când', 'care', 'cărei', 'căror', 'cărui', 'cât', 'câte', 'câţi', 'către', 'câtva', 'caut', 'ce', 'cel', 'ceva', 'chiar', 'cinci', 'cînd', 'cine', 'cineva', 'cît', 'cîte', 'cîţi', 'cîtva', 'contra', 'cu', 'cum', 'cumva', 'curând', 'curînd', 'da', 'dă', 'dacă', 'dar', 'dată', 'datorită', 'dau', 'de', 'deci', 'deja', 'deoarece', 'departe', 'deşi', 'din', 'dinaintea', 'dintr-', 'dintre', 'doi', 'doilea', 'două', 'drept', 'după', 'ea', 'ei', 'el', 'ele', 'eram', 'este', 'eşti', 'eu', 'face', 'fără', 'fata', 'fi', 'fie', 'fiecare', 'fii', 'fim', 'fiţi', 'fiu', 'frumos', 'graţie', 'halbă', 'iar', 'ieri', 'îi', 'îl', 'îmi', 'împotriva', 'în', 'înainte', 'înaintea', 'încât', 'încît', 'încotro', 'între', 'întrun', 'întruna', 'întrucât', 'întrucît', 'îţi', 'la', 'lângă', 'le', 'li', 'lîngă', 'lor', 'lui', 'mă', 'mai', 'mâine', 'mea', 'mei', 'mele', 'mereu', 'meu', 'mi', 'mie', 'mîine', 'mine', 'mult', 'multă', 'mulţi', 'mulţumesc', 'ne', 'nevoie', 'nicăieri', 'nici', 'nimeni', 'nimeri', 'nimic', 'nişte', 'noastră', 'noastre', 'noi', 'noroc', 'noştri', 'nostru', 'nouă', 'nu', 'opt', 'ori', 'oricând', 'oricare', 'oricât', 'orice', 'oricînd', 'oricine', 'oricît', 'oricum', 'oriunde', 'până', 'patra', 'patru', 'patrulea', 'pe', 'pentru', 'peste', 'pic', 'pînă', 'poate', 'pot', 'prea', 'prima', 'primul', 'prin', 'puţin', 'puţina', 'puţină', 'rog', 'sa', 'să', 'săi', 'sale', 'şapte', 'şase', 'sau', 'său', 'se', 'şi', 'sînt', 'sîntem', 'sînteţi', 'spate', 'spre', 'ştiu', 'sub', 'sunt', 'suntem', 'sunteţi', 'sută', 'ta', 'tăi', 'tale', 'tău', 'te', 'ţi', 'ţie', 'timp', 'tine', 'toată', 'toate', 'tot', 'toţi', 'totuşi', 'trei', 'treia', 'treilea', 'tu', 'un', 'una', 'unde', 'undeva', 'unei', 'uneia', 'unele', 'uneori', 'unii', 'unor', 'unora', 'unu', 'unui', 'unuia', 'unul', 'vă', 'vi', 'voastră', 'voastre', 'voi', 'voştri', 'vostru', 'vouă', 'vreme', 'vreo', 'vreun', 'zece', 'zero', 'zi', 'zice']\n","\n","\n","def count_num_stopwords(text):\n","    result = preprocess(text)\n","    words = result.split()\n","    count = len([word for word in words if word not in romanian_stopwords])\n","    return count\n","\n","\n","def extract_data(df):\n","    commons = []\n","    propers = []\n","    proper_words_title = []\n","    f_measures = []\n","    cls_scores = []\n","    for index, row in df.iterrows():\n","        count_common, count_proper, num_proper_words_title, f_measure, cls_score = extract_data_from_pos_tags(\n","            row['title'], row['content'])\n","        commons.append(count_common)\n","        propers.append(count_proper)\n","        proper_words_title.append(num_proper_words_title)\n","        f_measures.append(f_measure)\n","        cls_scores.append(cls_score)\n","\n","    return commons, propers, proper_words_title, f_measures, cls_scores\n","\n","\n","def create_data_frame(df):\n","    new_df = pd.DataFrame()\n","\n","    # independent title features\n","    new_df[\"processed_title\"] = df['title'].apply(preprocess)\n","    new_df[\"is_question\"] = df['title'].apply(isquestion)\n","    new_df[\"num_words\"] = df['title'].apply(lambda x: len(x.split()))\n","    new_df[\"rix_title\"] = df['title'].apply(compute_RIX)\n","    new_df[\"lix_title\"] = df['title'].apply(compute_LIX)\n","    new_df[\"num_stopwords\"] = df['title'].apply(count_num_stopwords)\n","    new_df[\"punct_patterns\"] = df[\"title\"].apply(punctuation_patterns)\n","    new_df[\"stop_word_ratio\"] = new_df['num_stopwords'] / new_df['num_words']\n","\n","    # title + content common features\n","    commons, propers, proper_words_title, f_measures, cls_scores = extract_data(df)\n","    new_df[\"num_proper_words\"] = proper_words_title\n","    new_df[\"fmeasure_title\"] = f_measures\n","    new_df[\"clscore\"] = cls_scores\n","    new_df[\"commons\"] = commons\n","    new_df[\"propers\"] = propers\n","\n","    # content features\n","    new_df[\"rix_content\"] = df['content'].apply(compute_RIX)\n","    new_df[\"lix_content\"] = df['content'].apply(compute_LIX)\n","\n","    return new_df\n","\n","\n","# RIX = num_long_words / num_sentences\n","\n","def compute_RIX(text):\n","    number_of_sentences = len(sent_tokenize(text))\n","    result = preprocess(text)\n","    words = result.split()\n","\n","    words = [word.lower() for word in words]\n","    words = [word for word in words if len(word) > 7]\n","\n","    rix = 0\n","\n","    if number_of_sentences != 0:\n","        rix = len(words) / float(number_of_sentences)\n","    else:\n","        rix = 0\n","\n","    return rix\n","\n","\n","# LIX = num_words / num_sentences + (100 * num_long_words) / num_words\n","def compute_LIX(text):\n","    number_of_sentences = len(sent_tokenize(text))\n","    result = preprocess(text)\n","    words = result.split()\n","\n","    words = [word.lower() for word in words]\n","    w = len(words)\n","    words = [word for word in words if len(word) > 7]\n","    long_words = len(words)\n","\n","    v1 = v2 = 0\n","    if number_of_sentences != 0:\n","        v1 = w / float(number_of_sentences)\n","\n","    if w != 0:\n","        v2 = (100 * long_words) / float(w)\n","\n","    lix = v1 + v2\n","\n","    return lix\n","\n","\n","def extract_data_from_pos_tags(title, content):\n","    title_doc = nlp(title)\n","    text_doc = nlp(content)\n","\n","    commons = set()\n","    propers = set()\n","\n","    noun_freq = 0\n","    adj_freq = 0\n","    prep_freq = 0\n","    article_freq = 0\n","    pronoun_freq = 0\n","    verb_freq = 0\n","    adv_freq = 0\n","    interj_freq = 0\n","\n","    num_proper_words_title = 0\n","\n","    for sent in title_doc.sentences:\n","        for word in sent.words:\n","            if word.upos == 'PROPN':\n","                propers.add(word.text)\n","                num_proper_words_title += 1\n","            elif word.upos == 'NOUN':\n","                commons.add(word.text)\n","\n","            pos = word.upos\n","            if word.text.lower() in ['oh', 'wow', 'hmm', 'uh', 'um']:\n","                interj_freq += 1\n","            elif \"NOUN\" in pos:\n","                noun_freq += 1\n","            elif \"ADJ\" in pos:\n","                adj_freq += 1\n","            elif \"ADP\" in pos:\n","                prep_freq += 1\n","            elif word.text.lower() in ['un', 'o', 'niște', 'acest', 'această', 'acești', 'aceste', 'al', 'ai', 'ale']:\n","                article_freq += 1\n","            elif 'PRON' in pos and not 'PUNCT' in pos:\n","                pronoun_freq += 1\n","            elif 'VERB' in pos:\n","                verb_freq += 1\n","            elif 'ADV' in pos:\n","                adv_freq += 1\n","\n","    count_common = 0\n","    count_proper = 0\n","\n","    num_words = 0\n","    num_sentences = 0\n","    num_long_words = 0\n","    len_words = 0\n","    for sent in text_doc.sentences:\n","        num_sentences += 1\n","        for word in sent.words:\n","            num_words += 1\n","            len_words += len(word.text)\n","            if word.text in propers and word.upos == 'PROPN':\n","                count_proper += 1\n","            elif word.text in commons and word.upos == 'NOUN':\n","                count_common += 1\n","\n","            if len(word.text) > 7:\n","                num_long_words += 1\n","\n","    f_measure = (noun_freq + adj_freq + prep_freq + article_freq) / 2 - (\n","                pronoun_freq + verb_freq + adv_freq + interj_freq + 100) / 2\n","\n","    if num_words != 0:\n","        avg_letters_per_100_words = len_words / num_words * 100\n","        avg_sentences_per_100_words = num_sentences / num_words * 100\n","    else:\n","        avg_letters_per_100_words = 0\n","        avg_sentences_per_100_words = 0\n","\n","    cls_score = 0.0588 * avg_letters_per_100_words - 0.296 * num_sentences - 15.8\n","\n","    return count_common, count_proper, num_proper_words_title, f_measure, cls_score\n","\n","\n","def punctuation_patterns(title):\n","    patterns = ['!?', '...', '***', '!!!', '???', '(', ')', '$']\n","\n","    found = False\n","    counts = {}\n","    for pattern in patterns:\n","        if pattern in title:\n","            found = True\n","            break\n","\n","    return found\n","\n","\n","def get_pos_title(title):\n","    title_doc = nlp(title)\n","    pos_result = []\n","    for sent in title_doc.sentences:\n","        for word in sent.words:\n","            pos_result.append(word.upos)\n","\n","    result = \" \".join(pos_result)\n","    return result\n","\n","\n","def rfc_predict_article(title, content, rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val,rfc_model):\n","    df = pd.DataFrame({\n","        \"title\": title,\n","        \"content\": content\n","    }, index=[1])\n","\n","    df_processed = create_data_frame(df)\n","    df_processed[\"pos_title\"] = df['title'].apply(get_pos_title)\n","\n","    x_test_headline = rfc_tfidf.transform(df_processed['processed_title'])\n","\n","    x_test_pos = rfc_cv_pos.transform(df_processed['pos_title'])\n","    x_test_pos_sc = rfc_sc_pos.transform(x_test_pos)\n","\n","    x_test_val = df_processed.drop(columns=['processed_title', 'pos_title']).values\n","\n","    x_test_val_sc = rfc_sc_val.transform(x_test_val)\n","\n","    x_test = sparse.hstack([x_test_headline, x_test_pos_sc, x_test_val_sc]).tocsr()\n","\n","    label = rfc_model.predict(x_test)[0].item()\n","\n","    return label\n","\n","\n","def load_rfc(folder_path):\n","    rfc_tfidf_path  = folder_path + \"tfidf.pkl\"\n","    rfc_cv_pos_path = folder_path + \"cv_pos.pkl\"\n","    rfc_sc_pos_path = folder_path + \"sc_pos.pkl\"\n","    rfc_sc_val_path = folder_path + \"sc_val.pkl\"\n","    rfc_model_path  = folder_path + \"rfc_model.pkl\"\n","\n","    tfidf_path = rfc_tfidf_path\n","    with open(tfidf_path, 'rb') as f:\n","        rfc_tfidf = pickle.load(f)\n","\n","    cv_pos_path = rfc_cv_pos_path\n","    sc_pos_path = rfc_sc_pos_path\n","\n","    with open(cv_pos_path, 'rb') as f:\n","        rfc_cv_pos = pickle.load(f)\n","\n","    with open(sc_pos_path, 'rb') as f:\n","        rfc_sc_pos = pickle.load(f)\n","\n","    sc_val_path = rfc_sc_val_path\n","\n","    with open(sc_val_path, 'rb') as f:\n","        rfc_sc_val = pickle.load(f)\n","\n","    with open(rfc_model_path, 'rb') as f:\n","        rfc_model = pickle.load(f)\n","\n","    return rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model\n"],"metadata":{"id":"QAgysP23N-23"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LSTM"],"metadata":{"id":"NSP87YBwN-R_"}},{"cell_type":"code","source":["from keras.models import load_model\n","import pickle\n","from keras.utils import pad_sequences\n","import numpy as np\n","\n","def load_lstm_model(folder_path):\n","    lstm_model_path             = folder_path + \"bilstm_22mai/\" + \"bilstm_model\"\n","    lstm_title_tokenizer_path   = folder_path + \"bilstm_22mai/\" + \"tokenizer_title.pickle\"\n","    lstm_content_tokenizer_path = folder_path + \"bilstm_22mai/\" + \"tokenizer_content.pickle\"\n","\n","    lstm_model = load_model(lstm_model_path)\n","    with open(lstm_title_tokenizer_path, 'rb') as handle:\n","        loaded_tokenizer_title = pickle.load(handle)\n","\n","    with open(lstm_content_tokenizer_path, 'rb') as handle:\n","        loaded_tokenizer_content = pickle.load(handle)\n","\n","    return lstm_model, loaded_tokenizer_title, loaded_tokenizer_content\n","\n","def lstm_predict_article(title, text, lstm_model, loaded_tokenizer_title, loaded_tokenizer_content):\n","    encoded_title = loaded_tokenizer_title.texts_to_sequences([title])\n","    encoded_text = loaded_tokenizer_content.texts_to_sequences([text])\n","\n","    max_length_title = 49\n","    max_length_content = 9401\n","    padded_title = pad_sequences(encoded_title, maxlen=max_length_title, padding='post')\n","    padded_text = pad_sequences(encoded_text, maxlen=max_length_content, padding='post')\n","\n","    class_probabilities = lstm_model.predict([padded_title, padded_text])[0]\n","\n","    predicted_label = np.argmax(class_probabilities)\n","    return predicted_label"],"metadata":{"id":"u6J_1t_5OuE3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BERT"],"metadata":{"id":"KKN2RMf7OBNP"}},{"cell_type":"code","source":["! pip install transformers==4.28.0\n","! pip install datasets"],"metadata":{"id":"wHVju6XqQBKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from transformers import BertTokenizerFast, BertForSequenceClassification\n","import pandas as pd\n","from datasets import Dataset\n","\n","\n","class CustomClassifier(nn.Module):\n","    def __init__(self, pretrained_model, num_classes):\n","        super(CustomClassifier, self).__init__()\n","        self.base_model = pretrained_model\n","        self.dropout = nn.Dropout(0.2)\n","        self.dense = nn.Linear(self.base_model.config.hidden_size, 128)\n","        self.classifier = nn.Linear(128, num_classes)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n","        hidden_state = outputs.last_hidden_state\n","        x = hidden_state[:, 0, :]\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.relu(x)\n","        x = self.classifier(x)\n","        return x\n","\n","    def predict(self, input_ids, attention_mask):\n","        outputs = self.forward(input_ids=input_ids, attention_mask=attention_mask)\n","        predictions = torch.argmax(outputs, dim=1)\n","        return predictions\n","\n","\n","def load_bert_model(folder_path):\n","    bert_model_path      = folder_path + \"trainer\"\n","    bert_clasifier_path  = folder_path + \"custom_classifier.pth\"\n","\n","    bert_model = BertForSequenceClassification.from_pretrained(bert_model_path)\n","    classifier_model = CustomClassifier(bert_model.base_model, 2)\n","    # classifier_model.load_state_dict(torch.load(bert_clasifier_path))\n","    classifier_model.load_state_dict(torch.load(bert_clasifier_path, map_location=torch.device('cpu')))\n","\n","    model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n","    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n","\n","    return classifier_model, tokenizer\n","\n","\n","def predict_labels(model, dataset, device):\n","    model.eval()\n","    dataloader = DataLoader(dataset, batch_size=8)\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","\n","            outputs = model.predict(input_ids=input_ids, attention_mask=attention_mask)\n","            predictions.extend(outputs.cpu().numpy())\n","\n","    return predictions\n","\n","\n","def preprocess_dataset(dataset, tokenizer):\n","    def tokenize_and_merge_title_content(examples):\n","        texts = [f\"{title} {tokenizer.sep_token} {article}\" for title, article in zip(examples['title'], examples['content'])]\n","        tokenized = tokenizer.batch_encode_plus(\n","            texts,\n","            max_length=512,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_token_type_ids=False,\n","        )\n","        return {\n","            \"input_ids\": tokenized[\"input_ids\"],\n","            \"attention_mask\": tokenized[\"attention_mask\"],\n","            \"labels\": examples[\"category\"]\n","        }\n","\n","    dataset = dataset.map(tokenize_and_merge_title_content, batched=True)\n","    dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n","    return dataset\n","\n","\n","def bert_predict_article(title, text, classifier_model, tokenizer):\n","    df_test = pd.DataFrame({'title': [title], 'content': [text], 'category': [0]})\n","    test_dataset = Dataset.from_pandas(df_test)\n","\n","    tokenized_dataset = preprocess_dataset(test_dataset, tokenizer)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    label = predict_labels(classifier_model, tokenized_dataset, device)\n","\n","    return label[0]\n","\n"],"metadata":{"id":"V6kkIohuN6Kn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Contrastive Learning model"],"metadata":{"id":"JPv3pqtDOFLX"}},{"cell_type":"code","source":["from transformers import BertTokenizerFast\n","from torch.nn.functional import cosine_similarity\n","from transformers import BertModel\n","import re\n","from string import punctuation\n","import torch\n","from torch import nn\n","\n","\n","def preprocess(text):\n","  result = text.replace('/',\"\").replace('\\n','')\n","  result = re.sub(r'[0-9]+','număr',result)\n","  result = re.sub(r'(\\w)(\\1{2,})',r'\\1',result)\n","  result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n","  result = result.lower()\n","  punctuations = punctuation + \"„”\"\n","  result = \"\".join(word for word in result if word not in punctuations)\n","  re.sub(r' +',' ',result).lower().strip()\n","  return result\n","\n","\n","class SiameseNetwork(nn.Module):\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","        self.text_encoder = BertModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","        outputs = self.text_encoder(input_ids=input_ids,\n","                                    attention_mask=attention_mask,\n","                                    token_type_ids=token_type_ids)\n","        # last hidden state for bert\n","        last_hidden_state = outputs.last_hidden_state\n","\n","        # mean pooling\n","\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()  # expand mask\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded,\n","                                   1)  # multiply last hidden state by mask to ignore padding tokens\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9)  # sum mask != 0\n","        mean_embeddings = sum_embeddings / sum_mask  # mean token embeddings\n","        return mean_embeddings\n","\n","\n","def contrastive_predict_article(title, text, model, threshold=0.25):\n","    model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n","    tokenizer = BertTokenizerFast.from_pretrained(model_name)\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    title = preprocess(title)\n","    text = preprocess(text)\n","\n","    inputs_title = tokenizer(title, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","    inputs_text = tokenizer(text, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","\n","    output1 = model(**inputs_title)\n","    output2 = model(**inputs_text)\n","\n","    cos_sim = cosine_similarity(output1, output2)\n","\n","    predicted_similarity = (1.0 - cos_sim) < threshold\n","\n","    if predicted_similarity == False:  # clickbait\n","        return 1\n","    else:\n","        return 0\n","\n","\n","def load_contrastive_model(folder_path):\n","    contrastive_path = folder_path + \"model_contrastive_learning.pt\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    model_path = contrastive_path\n","    model = SiameseNetwork()\n","    model = model.to(device)\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","\n","    return model\n"],"metadata":{"id":"pb8WrrpkOJnY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ensemble Prediction"],"metadata":{"id":"sMTlDpfaOJ9R"}},{"cell_type":"code","source":["def load_all_models():\n","    # Load Bert model\n","    classifier_model, tokenizer = load_bert_model(BERT_PATH)\n","\n","    # Load LSTM model\n","    lstm_model, loaded_tokenizer_title, loaded_tokenizer_content = load_lstm_model(LSTM_PATH)\n","\n","    # Load RFC model\n","    rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model = load_rfc(RFC_PATH)\n","\n","    # Load Contrastive model\n","    contrastive_model = load_contrastive_model(CONTRASTIVE_PATH)\n","\n","    return classifier_model, tokenizer, lstm_model, loaded_tokenizer_title, loaded_tokenizer_content, rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model, contrastive_model\n","\n","\n","def combine_predict_article(title, content, classifier_model, tokenizer, lstm_model, loaded_tokenizer_title,\n","                            loaded_tokenizer_content, rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model,\n","                            contrastive_model):\n","    lstm_label = lstm_predict_article(title, content, lstm_model, loaded_tokenizer_title, loaded_tokenizer_content)\n","    rfc_label = rfc_predict_article(title, content, rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model)\n","    bert_label = bert_predict_article(title, content, classifier_model, tokenizer)\n","    contrastive_label = contrastive_predict_article(title, content, contrastive_model)\n","\n","    predicted = 0.27 * lstm_label + 0.24 * rfc_label + 0.24 * bert_label + 0.25 * contrastive_label\n","\n","    if predicted >= 0.49:\n","        predicted = 1\n","    else:\n","        predicted = 0\n","    return predicted, bert_label, lstm_label, rfc_label, contrastive_label"],"metadata":{"id":"_TjUhhSHRBpV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import os\n","import pandas as pd\n","\n","def read_file(path, name):\n","  file_path = path + \"/\" + name \n","\n","  reader = open(file_path)\n","  json_array = json.load(reader)\n","  news = []\n","  # nonclickbait = 0\n","  # clickbait = 1\n","\n","  for element in json_array:\n","    cat = 1\n","    if element[\"category\"] == \"nonclickbait\":\n","      cat = 0\n","    item = {\n","        \"title\":element[\"title\"],\n","        \"content\":element[\"content\"],\n","        \"category\":cat\n","            }\n","    news.append(item)\n","\n","  return news\n","\n","def read_raw_data(folder_path):\n","  filenames = sorted(os.listdir(folder_path))\n","\n","  raw_data = []\n","  for filename in filenames:\n","    print(filename)\n","    current = read_file(folder_path, filename)\n","    raw_data.extend(current)\n","\n","  return raw_data"],"metadata":{"id":"Z7e1mNyKVYKM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Test files:')\n","test_raw_data  = read_raw_data(TEST_PATH)\n","print(\"---------------------\")\n","df_test = pd.DataFrame(test_raw_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tjinEJkvVfl8","executionInfo":{"status":"ok","timestamp":1685982233627,"user_tz":-180,"elapsed":2,"user":{"displayName":"Daria Broscoteanu","userId":"17692645243194726821"}},"outputId":"3957b3b0-adcc-410d-d454-8fe979c93f14"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test files:\n","digi.json\n","libertatea.json\n","---------------------\n"]}]},{"cell_type":"code","source":["classifier_model, tokenizer, lstm_model, loaded_tokenizer_title, loaded_tokenizer_content, rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model, contrastive_model = load_all_models()"],"metadata":{"id":"z3t_VXQZX5Ud"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["true_labels = []\n","pred_labels = []\n","\n","for index, row in df_test.iterrows():\n","    title = row['title']\n","    content = row['content']\n","    label = row[\"category\"]\n","    true_labels.append(label)\n","\n","    predicted = combine_predict_article(title, content, classifier_model, tokenizer, lstm_model, loaded_tokenizer_title,\n","                            loaded_tokenizer_content, rfc_tfidf, rfc_cv_pos, rfc_sc_pos, rfc_sc_val, rfc_model,\n","                            contrastive_model)\n","    \n","    pred_labels.append(predicted)"],"metadata":{"id":"JdntDBO8RazW"},"execution_count":null,"outputs":[]}]}