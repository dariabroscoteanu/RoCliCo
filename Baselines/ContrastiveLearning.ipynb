{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMY+CMdN6kDQtzNo/nhVy0C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Please fill the following paths."],"metadata":{"id":"w11q_HH59fqv"}},{"cell_type":"code","source":["# Path to the test dataset, containing \"digi.json\" and \"libertatea.json\"\n","TEST_PATH = \"\"\n","\n","# Path to the test dataset, containing \"protv.json\", \"cancan.json\" and \"wowbiz.json\"\n","TRAIN_PATH = \"\"\n","\n","# Path to the folder in which the model and other necessary tools are saved\n","FOLDER_PATH = \"\""],"metadata":{"id":"KgI17Mr99eWH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imports."],"metadata":{"id":"Fy62BSCd1-_V"}},{"cell_type":"code","source":["! pip install transformers==4.28.0"],"metadata":{"id":"Rj-PCEnH2BLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install datasets"],"metadata":{"id":"ZUwu57Pn2CeM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n","import json \n","import pandas as pd\n","import os\n","from datasets import load_dataset\n","\n","from string import punctuation\n","import re\n","\n","import torch\n","import torch.nn.functional as F\n","from torch import nn\n","\n","from torch.utils.data import DataLoader\n","\n","from transformers import BertModel\n","from torch.nn.functional import cosine_similarity\n","\n","from datasets import Dataset"],"metadata":{"id":"xwzx5nSD2Ezq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reading test and train datasets."],"metadata":{"id":"a4R8TK2t2Is2"}},{"cell_type":"code","source":["def preprocess(text):\n","  result = text.replace('/',\"\").replace('\\n','')\n","  result = re.sub(r'[0-9]+','număr',result)\n","  result = re.sub(r'(\\w)(\\1{2,})',r'\\1',result)\n","  result = re.sub(r'(?x)\\b(?=\\w*\\d)\\w+\\s*', '', result)\n","  result = result.lower()\n","  punctuations = punctuation + \"„”\"\n","  result = \"\".join(word for word in result if word not in punctuations)\n","  result = ' '.join(result.split())\n","  re.sub(r' +',' ',result).lower().strip()\n","  return result"],"metadata":{"id":"99O_GLwr2Muh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_file(path, name):\n","  file_path = path + \"/\" + name \n","\n","  reader = open(file_path)\n","  json_array = json.load(reader)\n","  news = []\n","  # nonclickbait = 1\n","  # clickbait = 0\n","\n","  for element in json_array:\n","    cat = 0\n","    if element[\"category\"] == \"nonclickbait\":\n","      cat = 1\n","    item = {\n","        \"title\" : preprocess(element[\"title\"]),\n","        \"content\" : preprocess(element[\"content\"]),\n","        \"category\":cat\n","            }\n","    news.append(item)\n","\n","  return news"],"metadata":{"id":"M8ki9Vgz2OOr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_raw_data(folder_path):\n","  filenames = sorted(os.listdir(folder_path))\n","\n","  raw_data = []\n","  for filename in filenames:\n","    print(filename)\n","    current = read_file(folder_path, filename)\n","    raw_data.extend(current)\n","\n","  return raw_data"],"metadata":{"id":"zC7S2YZm2UXi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Test files:')\n","test_raw_data  = read_raw_data(TEST_PATH)\n","print(\"---------------------\")\n","print('Train files:')\n","train_raw_data = read_raw_data(TRAIN_PATH)\n","print(\"---------------------\")"],"metadata":{"id":"VKr3pHKl2V-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.DataFrame(train_raw_data)\n","df_test = pd.DataFrame(test_raw_data)"],"metadata":{"id":"nzk_bJQE2aNj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model"],"metadata":{"id":"QZeusd0L2drJ"}},{"cell_type":"code","source":["model_name = \"dumitrescustefan/bert-base-romanian-cased-v1\"\n","tokenizer = BertTokenizerFast.from_pretrained(model_name)"],"metadata":{"id":"Bzv3M4Ju2fVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SiameseNetwork(nn.Module):\n","    def __init__(self):\n","        super(SiameseNetwork, self).__init__()\n","        self.text_encoder = BertModel.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\")\n","\n","    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n","        outputs = self.text_encoder(input_ids=input_ids,\n","                                    attention_mask=attention_mask,\n","                                    token_type_ids=token_type_ids)\n","\n","        last_hidden_state = outputs.last_hidden_state\n","        \n","        # mean pooling \n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float() # expand mask\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1) # multiply last hidden state by mask to ignore padding tokens\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min=1e-9) # sum mask != 0\n","        mean_embeddings = sum_embeddings / sum_mask # mean token embeddings\n","        return mean_embeddings"],"metadata":{"id":"sdFxsI-b2kcy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ContrastiveLoss(nn.Module):\n","    def __init__(self, margin=1.0):\n","        super().__init__()\n","        self.margin = margin\n","\n","    def forward(self, output1, output2, target):\n","        \n","        distances = 1.0 - cosine_similarity(output1, output2) # dissimilarity\n","        # distances: 0 - identical\n","        # distances: 1 - dissimilar\n","        \n","        # target == 1, just distance - to penalize the dissimilar pairs\n","        # target = 0, relu(margin - distance) - if the pair is dissimilar, but the distance is < margin, it contributes to the loss, \n","                                                # otherwise, the result is 0 so it doesnt contribute\n","        loss = 0.5 * (target.float() * distances +\n","                      (1.0 - target).float() * torch.relu(self.margin - distances).float())\n","        return loss.mean()"],"metadata":{"id":"GMmAQDT42moh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset =  Dataset.from_pandas(df_train)\n","hf_dataset_splits = dataset.train_test_split(test_size=0.3)\n","train_dataset = hf_dataset_splits['train']\n","validation_dataset =  hf_dataset_splits['test']\n","\n","test_dataset = Dataset.from_pandas(df_test)"],"metadata":{"id":"flf3eW5h2voa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 5\n","batch_size = 4\n","\n","model = SiameseNetwork().to(device)\n","criterion = ContrastiveLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)"],"metadata":{"id":"QMkOQ_vA2xvD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(num_epochs):\n","    print(f'Starting epoch {epoch+1}/{num_epochs}')\n","    \n","    running_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","    \n","    model.train()\n","    \n","    for batch in train_dataloader:\n","        title, content, target = batch['title'], batch['content'], batch['category']\n","        \n","        inputs_title = tokenizer(title, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","        inputs_text = tokenizer(content, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","        \n","        optimizer.zero_grad()\n","        \n","        output1 = model(**inputs_title) \n","        output2 = model(**inputs_text)\n","        target = target.to(device)\n","        loss = criterion(output1, output2, target)\n","       \n","        loss.backward()\n","        optimizer.step() \n","        \n","        running_loss += loss.item()\n","        \n","        cos_sim = cosine_similarity(output1, output2)\n","        predicted_similarity = (1.0 - cos_sim) < 0.25\n","        correct_predictions += accuracy_score(target.cpu(), predicted_similarity.cpu().type(torch.LongTensor), normalize=False)\n","        total_predictions += target.shape[0]\n","        \n","    avg_loss_train = running_loss / len(train_dataloader)\n","    accuracy_train = correct_predictions / total_predictions  \n","                         \n","    print(f'Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_loss_train}, Train Accuracy: {accuracy_train}')\n","               \n","    model.eval()\n","    running_loss = 0\n","    correct_predictions = 0\n","    total_predictions = 0\n","                         \n","    with torch.no_grad():\n","        for batch in validation_dataloader:\n","            title, content, target = batch['title'], batch['content'], batch['category']\n","\n","            inputs_title = tokenizer(title, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","            inputs_text = tokenizer(content, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","            \n","            output1 = model(**inputs_title) \n","            output2 = model(**inputs_text)\n","            target = target.to(device)\n","            loss = criterion(output1, output2, target)\n","\n","            running_loss += loss.item()\n","\n","            cos_sim = cosine_similarity(output1, output2)\n","            predicted_similarity = (1.0 - cos_sim) < 0.25\n","            correct_predictions += accuracy_score(target.cpu(), predicted_similarity.cpu().type(torch.LongTensor), normalize=False)\n","            total_predictions += target.shape[0]\n","\n","    avg_val_loss = running_loss / len(validation_dataloader)\n","    val_accuracy = correct_predictions / total_predictions\n","                                         \n","    print(f'Epoch {epoch+1}/{num_epochs} - Val Loss: {avg_val_loss}, Val Accuracy: {val_accuracy}')"],"metadata":{"id":"AQmt9cwg2_eJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n","\n","running_loss = 0.0\n","correct_predictions = 0\n","total_predictions = 0\n","\n","all_predictions = []\n","all_labels = []\n","\n","model.eval()\n","\n","with torch.no_grad():\n","    for batch in test_dataloader:\n","        title, content, target = batch['title'], batch['content'], batch['category']\n","        target = target.to(device)\n","\n","        inputs_title = tokenizer(title, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","        inputs_text = tokenizer(content, padding=True, truncation=True, max_length=256, return_tensors='pt').to(device)\n","            \n","        output1 = model(**inputs_title) \n","        output2 = model(**inputs_text)\n","        \n","        loss = criterion(output1, output2, target)\n","        running_loss += loss.item()\n","\n","        predicted_similarity = (1.0 - cosine_similarity(output1, output2)) < 0.25\n","        correct_predictions += accuracy_score(target.cpu(), predicted_similarity.cpu().type(torch.LongTensor), normalize=False)\n","        total_predictions += target.shape[0]\n","\n","        all_predictions.extend(predicted_similarity.cpu().numpy())\n","        all_labels.extend(target.cpu().numpy())\n","        \n","avg_test_loss = running_loss / len(test_dataloader)\n","test_accuracy = correct_predictions / total_predictions\n","\n","print(f'Test: Loss: {avg_test_loss}, Accuracy: {test_accuracy}')"],"metadata":{"id":"Ml5mRap64BMo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_path = FOLDER_PATH + \"model_contrastive_learning.pt\"\n","torch.save(model.state_dict(), model_path)"],"metadata":{"id":"ek5ppVa04FSg"},"execution_count":null,"outputs":[]}]}